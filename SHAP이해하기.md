# ML-study

기계학습 모델을 하다 보면, 더 복잡하고 강력한 예측 모델들이 등장하고 있다. 이러한 알고리즘들을 실행하다보면 자연스레 "이 학습 과정에서 어떠한 일이 발생했을까?"란 궁금증이 한 번씩 들었을 것이다.

이러한 의문은 학습 과정에서 그 과정을 확인하기 어렵다는 것에서부터 시작한다. 따라서 이러한 "블랙박스"는 현재 머신러닝의 해석 가능성과 설명 가능성에 대한 관심이 높아지는 지금 중요한 쟁점이 되었다. 이것을 해결하기 위해 유망한 도구 중 하나로 SHAP(SHapley Additive exPlanations)가 있다. 

SHAP는
